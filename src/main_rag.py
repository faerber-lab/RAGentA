import numpy as np
from tqdm import tqdm

class MAIN_RAG:
    def __init__(self, retriever, agent_model="mistralai/Mistral-7B-v0.1", n=0.0):
        """
        MAIN-RAG framework implementation.
        
        Args:
            retriever: Document retriever instance
            agent_model: Model name for the agents
            n: Hyperparameter for adaptive judge bar adjustment
        """
        from src.agents import LLMAgent
        
        self.retriever = retriever
        # You can use the same model for all agents or different models
        self.agent1 = LLMAgent(agent_model)  # Predictor
        self.agent2 = self.agent1  # Judge (using same instance to save memory)
        self.agent3 = self.agent1  # Final-Predictor (using same instance to save memory)
        self.n = n  # Hyperparameter for adaptive judge bar adjustment
        
    def _create_agent1_prompt(self, query, document):
        """Create prompt for Agent-1 (Predictor)."""
        return f"""You are an accurate and reliable AI assistant that can answer questions with the help of external documents. You should only provide the correct answer without repeating the question and instruction.

Document: {document}

Question: {query}

Answer:"""
    
    def _create_agent2_prompt(self, query, document, answer):
        """Create prompt for Agent-2 (Judge)."""
        return f"""You are a noisy document evaluator that can judge if the external document is noisy for the query with unrelated or misleading information. Given a retrieved Document, a Question, and an Answer generated by an LLM (LLM Answer), you should judge whether both the following two conditions are reached: (1) the Document provides specific information for answering the Question; (2) the LLM Answer directly answers the question based on the retrieved Document. Please note that external documents may contain noisy or factually incorrect information. If the information in the document does not contain the answer, you should point it out with evidence. You should answer with "Yes" or "No" with evidence of your judgment, where "No" means one of the conditions (1) and (2) are unreached and indicates it is a noisy document.

Document: {document}

Question: {query}

LLM Answer: {answer}

Is this document relevant and supportive for answering the question?"""
    
    def _create_agent3_prompt(self, query, filtered_documents):
        """Create prompt for Agent-3 (Final-Predictor)."""
        docs_text = "\n\n".join([f"Document {i+1}: {doc}" for i, doc in enumerate(filtered_documents)])
        return f"""You are an accurate and reliable AI assistant that can answer questions with the help of external documents. Your task is to answer the following question using ONLY the information from the provided documents.

Provide a single, direct answer to the question. Do NOT include follow-up questions or additional Q&A pairs.

Documents:
{docs_text}

Question: {query}

Answer: """
    
    def answer_query(self, query):
        """
        Process a query using the MAIN-RAG framework.
        
        Args:
            query: The user query
            
        Returns:
            final_answer: The generated answer
            debug_info: Dictionary with debug information
        """
        print(f"Processing query: {query}")
        
        # Step 1: Retrieve documents
        print("Retrieving documents...")
        retrieved_docs = self.retriever.retrieve(query)
        print(f"Retrieved {len(retrieved_docs)} documents")
        
        # Step 2: Agent-1 generates answers for each document
        print("Agent-1 generating answers for each document...")
        doc_answers = []
        for doc in tqdm(retrieved_docs):
            prompt = self._create_agent1_prompt(query, doc)
            answer = self.agent1.generate(prompt)
            doc_answers.append((doc, answer))
        
        # Step 3: Agent-2 evaluates and scores each document
        print("Agent-2 evaluating and scoring documents...")
        scores = []
        judgments = []
        for doc, answer in tqdm(doc_answers):
            prompt = self._create_agent2_prompt(query, doc, answer)
            log_probs = self.agent2.get_log_probs(prompt, ["Yes", "No"])
            score = log_probs["Yes"] - log_probs["No"]
            scores.append(score)
            
            # For debugging, also get the full judgment
            judgment = self.agent2.generate(prompt)
            judgments.append(judgment)
        
        # Step 4: Calculate adaptive judge bar
        tau_q = np.mean(scores)
        sigma = np.std(scores)
        adjusted_tau_q = tau_q - self.n * sigma
        print(f"Adaptive judge bar: {tau_q:.4f}, adjusted: {adjusted_tau_q:.4f}")
        
        # Step 5: Filter and rank documents
        filtered_docs = []
        for i, (doc, _) in enumerate(doc_answers):
            if scores[i] >= adjusted_tau_q:
                filtered_docs.append((doc, scores[i]))
        
        # Sort by score in descending order
        filtered_docs.sort(key=lambda x: x[1], reverse=True)
        print(f"Filtered to {len(filtered_docs)} documents")
        
        # Step 6: Agent-3 generates final answer
        print("Agent-3 generating final answer...")
        if filtered_docs:
            docs_only = [doc for doc, _ in filtered_docs]
            prompt = self._create_agent3_prompt(query, docs_only)
            final_answer = self.agent3.generate(prompt)
        else:
            # Fall back to using all documents if none pass the filter
            print("Warning: No documents passed the filter, using all documents")
            docs_only = [doc for doc, _ in doc_answers]
            prompt = self._create_agent3_prompt(query, docs_only)
            final_answer = self.agent3.generate(prompt)
        
        # Return the answer and debug information
        debug_info = {
            "retrieved_docs": retrieved_docs,
            "doc_answers": [answer for _, answer in doc_answers],
            "scores": scores,
            "judgments": judgments,
            "tau_q": tau_q,
            "adjusted_tau_q": adjusted_tau_q,
            "filtered_docs": [(doc, score) for doc, score in filtered_docs]
        }
        
        return final_answer, debug_info
