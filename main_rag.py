import numpy as np
from tqdm import tqdm
import re
import copy


class MAIN_RAG:
    def __init__(
        self,
        retriever,
        agent_model=None,
        n=0.0,
        falcon_api_key=None,
        pinecone_api_key=None,
    ):
        """
        Enhanced MAIN-RAG framework implementation with citation tracking and judgment.

        Args:
            retriever: Document retriever instance (Pinecone or other)
            agent_model: Model name or pre-initialized agents
            n: Hyperparameter for adaptive judge bar adjustment (default 0.0)
            falcon_api_key: API key for Falcon model (if using Falcon)
            pinecone_api_key: API key for Pinecone (if not using a pre-initialized retriever)
        """
        self.retriever = retriever
        self.n = n  # Hyperparameter for adaptive judge bar adjustment

        # Save these for potential reuse in Agent4
        self.agent_model = agent_model
        self.falcon_api_key = falcon_api_key

        # Initialize agents based on provided parameters
        if isinstance(agent_model, str):
            if "falcon" in agent_model.lower() and falcon_api_key:
                # Initialize Falcon agents
                from falcon_agent import FalconAgent

                self.agent1 = FalconAgent(falcon_api_key)  # Predictor
                self.agent2 = (
                    self.agent1
                )  # Judge (reuse the same instance to save resources)
                self.agent3 = self.agent1  # Final-Predictor
                self.agent4 = self.agent1  # Claim Judge
                print(f"Using Falcon agents with API for all four agent roles")
            else:
                # Initialize local LLM agents
                from llm_agent import LLMAgent

                self.agent1 = LLMAgent(agent_model)  # Predictor
                self.agent2 = self.agent1  # Judge
                self.agent3 = self.agent1  # Final-Predictor
                self.agent4 = self.agent1  # Claim Judge
                print(f"Using local LLM agents with model {agent_model}")
        else:
            # Use pre-initialized agent
            self.agent1 = agent_model  # Predictor
            self.agent2 = self.agent1  # Judge
            self.agent3 = self.agent1  # Final-Predictor
            self.agent4 = self.agent1  # Claim Judge
            print("Using pre-initialized agent for all four agent roles")

    def _create_agent1_prompt(self, query, document):
        """Create prompt for Agent-1 (Predictor)."""
        return f"""You are an accurate and reliable AI assistant that can answer questions with the help of external documents. You should only provide the correct answer without repeating the question and instruction.

Document: {document}

Question: {query}

Answer:"""

    def _create_agent2_prompt(self, query, document, answer):
        """Create prompt for Agent-2 (Judge)."""
        return f"""You are a noisy document evaluator that can judge if the external document is noisy for the query with unrelated or misleading information. Given a retrieved Document, a Question, and an Answer generated by an LLM (LLM Answer), you should judge whether both the following two conditions are reached: (1) the Document provides specific information for answering the Question; (2) the LLM Answer directly answers the question based on the retrieved Document. Please note that external documents may contain noisy or factually incorrect information. If the information in the document does not contain the answer, you should point it out with evidence. You should answer with "Yes" or "No" with evidence of your judgment, where "No" means one of the conditions (1) and (2) are unreached and indicates it is a noisy document.

Document: {document}

Question: {query}

LLM Answer: {answer}

Is this document relevant and supportive for answering the question?"""

    def _create_agent3_prompt(self, query, filtered_documents):
        """
        Create prompt for Agent-3 (Final-Predictor) with citation instructions.
        Instructs the model to cite which document(s) support each claim.
        """
        docs_text = "\n\n".join(
            [f"Document {i+1}: {doc}" for i, doc in enumerate(filtered_documents)]
        )
        return f"""You are an accurate and reliable AI assistant that can answer questions with the help of external documents. Your answer should be based ONLY on the provided documents. If the documents don't contain sufficient information to answer the question, state that clearly.

IMPORTANT: For each claim or statement in your answer, you MUST include a citation to the specific document(s) that support it. Use the format [X] at the end of each sentence or claim, where X is the document number.

For example:
- If information comes from Document 1, end the sentence with [1].
- If a claim is supported by multiple documents, cite all of them like [1,3].
- Every factual claim must have a citation.

Documents:
{docs_text}

Question: {query}

Answer (with citations):"""

    def _create_agent4_prompt(self, query, answer_with_citations, all_documents):
        """
        Create prompt for Agent-4 (Claim Judge).
        This agent judges if the cited claims properly answer the question and identifies gaps.
        """
        docs_text = "\n\n".join(
            [f"Document {i+1}: {doc}" for i, doc in enumerate(all_documents)]
        )

        return f"""You are a meticulous judge evaluating if a question has been fully answered. Your task has three parts:

1. CLAIM ANALYSIS: Analyze the answer with citations and identify each claim made.
2. QUESTION COVERAGE: Determine if the original question has been completely answered.
3. GAP IDENTIFICATION: If parts of the question remain unanswered, identify these gaps precisely.

Original Question: {query}

Answer with Citations:
{answer_with_citations}

Available Documents:
{docs_text}

First, analyze each claim with its citation and evaluate if it correctly addresses part of the question.
Then, determine if any parts of the question remain unanswered.

Finally, provide your analysis in this format:
COMPLETELY_ANSWERED: Yes/No
UNANSWERED_ASPECTS: [List any aspects of the question that remain unanswered]
FOLLOW_UP_QUESTIONS: [If needed, rewrite specific questions to address the unanswered aspects]

Your analysis:"""

    def _create_follow_up_prompt(
        self, original_query, follow_up_question, filtered_documents, previous_answer
    ):
        """Create prompt for generating an answer to a follow-up question."""
        docs_text = "\n\n".join(
            [f"Document {i+1}: {doc}" for i, doc in enumerate(filtered_documents)]
        )

        return f"""You are an accurate and reliable AI assistant. Your task is to answer a follow-up question based on the provided documents.

Original Question: {original_query}
Previous Answer: {previous_answer}

Follow-up Question: {follow_up_question}

Documents:
{docs_text}

Answer the follow-up question ONLY based on the provided documents. Use the format [X] to cite which document(s) support each claim, where X is the document number. If the documents don't contain the necessary information, state that clearly.

Follow-up Answer (with citations):"""

    def _extract_claims_with_citations(self, answer_with_citations):
        """Extract individual claims and their citations from the answer."""
        # Simple regex-based extraction - can be enhanced for more complex answers
        claim_pattern = r"(.*?)\s*\[(\d+(?:,\s*\d+)*)\]"
        claims = []

        for match in re.finditer(claim_pattern, answer_with_citations):
            claim_text = match.group(1).strip()
            citation_str = match.group(2)
            citations = [int(c.strip()) for c in citation_str.split(",")]

            if claim_text:
                claims.append({"text": claim_text, "citations": citations})

        return claims

    def _combine_answers(self, original_answer, follow_up_answers):
        """Combine the original answer with follow-up answers."""
        if not follow_up_answers:
            return original_answer

        combined = original_answer + "\n\nAdditional information:\n"
        combined += "\n\n".join(follow_up_answers)

        return combined

    def clean_answer(self, answer):
        """Simple cleaning function to handle empty or problematic answers."""
        if not answer or answer.strip() == "":
            return "no answer provided"

        # Remove common problematic patterns
        patterns_to_remove = [
            r"^>.*$",  # Blog formatting
            r"^Post your responses.*$",  # Instructions
            r"^Source\(s\):.*$",  # Sources
            r"^Question:.*$",  # Follow-up questions
        ]

        for pattern in patterns_to_remove:
            answer = re.sub(pattern, "", answer, flags=re.MULTILINE)

        # Split by newlines and take first non-empty line
        lines = [line.strip() for line in answer.split("\n") if line.strip()]
        if not lines:
            return "no answer provided"

        return "\n".join(lines)  # Return all lines to preserve citations

    def answer_query(self, query, choices=None):
        """Process a query using the enhanced MAIN-RAG framework with citation and judgment."""
        print(f"Processing query: {query}")

        # Step 1: Retrieve documents
        print("Retrieving documents...")
        retrieved_docs = self.retriever.retrieve(
            query, top_k=20
        )  # Always retrieve 20 docs as in the paper
        print(f"Retrieved {len(retrieved_docs)} documents")

        # Step 2: Agent-1 generates answers for each document
        print("Agent-1 generating answers for each document...")
        doc_answers = []
        for doc in tqdm(retrieved_docs):
            prompt = self._create_agent1_prompt(query, doc)
            answer = self.agent1.generate(prompt)
            doc_answers.append((doc, answer))

        # Step 3: Agent-2 evaluates and scores each document
        print("Agent-2 evaluating and scoring documents...")
        scores = []
        for doc, answer in tqdm(doc_answers):
            prompt = self._create_agent2_prompt(query, doc, answer)
            log_probs = self.agent2.get_log_probs(prompt, ["Yes", "No"])
            score = log_probs["Yes"] - log_probs["No"]  # Key scoring mechanism
            scores.append(score)

        # Step 4: Calculate adaptive judge bar (τq)
        tau_q = np.mean(scores)
        sigma = np.std(scores)
        adjusted_tau_q = tau_q - self.n * sigma  # Use the n hyperparameter here
        print(f"Adaptive judge bar: τq={tau_q:.4f}, adjusted: {adjusted_tau_q:.4f}")

        # Step 5: Filter and rank documents
        filtered_docs = []
        filtered_indices = []  # Track which documents passed the filter
        for i, (doc, _) in enumerate(doc_answers):
            if scores[i] >= adjusted_tau_q:
                filtered_docs.append((doc, scores[i]))
                filtered_indices.append(i)

        # Sort by score in descending order (crucial for performance)
        filtered_docs.sort(key=lambda x: x[1], reverse=True)
        print(f"Filtered to {len(filtered_docs)} documents")

        # Step 6: Agent-3 generates answer with citations
        print("Agent-3 generating answer with citations...")
        if filtered_docs:
            docs_only = [doc for doc, _ in filtered_docs]
            prompt = self._create_agent3_prompt(query, docs_only)
            raw_answer = self.agent3.generate(prompt)
            answer_with_citations = self.clean_answer(raw_answer)
        else:
            # Fall back to using all documents if none pass the filter
            print("Warning: No documents passed the filter, using all documents")
            docs_only = [doc for doc, _ in doc_answers]
            prompt = self._create_agent3_prompt(query, docs_only)
            raw_answer = self.agent3.generate(prompt)
            answer_with_citations = self.clean_answer(raw_answer)

        # Step 7: Agent-4 judges if the question is completely answered
        print("Agent-4 judging answer completeness...")
        judge_prompt = self._create_agent4_prompt(
            query, answer_with_citations, docs_only
        )
        judge_response = self.agent4.generate(judge_prompt)

        # Extract judgment results
        completely_answered = "COMPLETELY_ANSWERED: Yes" in judge_response

        # Process follow-up questions if needed
        follow_up_answers = []

        if not completely_answered:
            print("Question not completely answered. Processing follow-up questions...")

            # Extract follow-up questions
            follow_up_match = re.search(
                r"FOLLOW_UP_QUESTIONS:(.*?)(?=COMPLETELY_ANSWERED:|$)",
                judge_response,
                re.DOTALL,
            )

            if follow_up_match:
                follow_up_text = follow_up_match.group(1).strip()
                follow_up_questions = [
                    q.strip() for q in follow_up_text.split("\n") if q.strip()
                ]

                # Get already retrieved docs to exclude
                already_retrieved_ids = set(doc_id for doc, _ in doc_answers)

                for i, follow_up_q in enumerate(follow_up_questions):
                    print(f"Processing follow-up question {i+1}: {follow_up_q}")

                    # Retrieve new documents for follow-up question (excluding already retrieved)
                    new_docs = self.retriever.retrieve(
                        follow_up_q, top_k=10, exclude_ids=already_retrieved_ids
                    )

                    if new_docs:
                        # Generate answer for follow-up question
                        follow_up_prompt = self._create_follow_up_prompt(
                            query, follow_up_q, new_docs, answer_with_citations
                        )
                        follow_up_answer = self.agent3.generate(follow_up_prompt)
                        follow_up_answers.append(follow_up_answer)

        # Combine original answer with follow-up answers
        final_answer = answer_with_citations
        if follow_up_answers:
            final_answer = self._combine_answers(
                answer_with_citations, follow_up_answers
            )

        # Make sure we never return empty answers
        if not final_answer or final_answer.strip() == "":
            final_answer = "no answer provided"

        # Return the answer and debug information
        debug_info = {
            "tau_q": tau_q,
            "adjusted_tau_q": adjusted_tau_q,
            "sigma": sigma,
            "scores": scores,
            "filtered_docs": [(doc, score) for doc, score in filtered_docs],
            "raw_answer": raw_answer,
            "answer_with_citations": answer_with_citations,
            "judge_response": judge_response,
            "completely_answered": completely_answered,
            "follow_up_answers": follow_up_answers,
        }

        return final_answer, debug_info

    def extract_multiple_choice_answer(self, text):
        """Extract just the letter choice from a multiple-choice answer."""
        if not text:
            return "B"  # Default to B if empty

        text = text.strip().upper()

        # If the text starts with A, B, C, or D, take just the letter
        if text and text[0] in "ABCD":
            return text[0]

        # Otherwise search for a letter in the text
        for char in text:
            if char in "ABCD":
                return char

        # Default to B if no letter found
        return "B"
