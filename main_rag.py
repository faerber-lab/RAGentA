import numpy as np
from tqdm import tqdm
import re
import copy
import logging

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("main_rag.log"), logging.StreamHandler()],
)
logger = logging.getLogger("MAIN_RAG")


class MAIN_RAG:
    def __init__(
        self,
        retriever,
        agent_model=None,
        n=0.0,
        falcon_api_key=None,
        pinecone_api_key=None,
    ):
        """
        Enhanced MAIN-RAG framework implementation with citation tracking and judgment.

        Args:
            retriever: Document retriever instance (Pinecone or other)
            agent_model: Model name or pre-initialized agents
            n: Hyperparameter for adaptive judge bar adjustment (default 0.0)
            falcon_api_key: API key for Falcon model (if using Falcon)
            pinecone_api_key: API key for Pinecone (if not using a pre-initialized retriever)
        """
        self.retriever = retriever
        self.n = n  # Hyperparameter for adaptive judge bar adjustment

        # Save these for potential reuse in Agent4
        self.agent_model = agent_model
        self.falcon_api_key = falcon_api_key

        # Initialize agents based on provided parameters
        if isinstance(agent_model, str):
            if "falcon" in agent_model.lower() and falcon_api_key:
                # Initialize Falcon agents
                from falcon_agent import FalconAgent

                self.agent1 = FalconAgent(falcon_api_key)  # Predictor
                self.agent2 = (
                    self.agent1
                )  # Judge (reuse the same instance to save resources)
                self.agent3 = self.agent1  # Final-Predictor
                self.agent4 = self.agent1  # Claim Judge
                logger.info(f"Using Falcon agents with API for all four agent roles")
            else:
                # Initialize local LLM agents
                from llm_agent import LLMAgent

                self.agent1 = LLMAgent(agent_model)  # Predictor
                self.agent2 = self.agent1  # Judge
                self.agent3 = self.agent1  # Final-Predictor
                self.agent4 = self.agent1  # Claim Judge
                logger.info(f"Using local LLM agents with model {agent_model}")
        else:
            # Use pre-initialized agent
            self.agent1 = agent_model  # Predictor
            self.agent2 = self.agent1  # Judge
            self.agent3 = self.agent1  # Final-Predictor
            self.agent4 = self.agent1  # Claim Judge
            logger.info("Using pre-initialized agent for all four agent roles")

    def _create_agent1_prompt(self, query, document_text):
        """Create prompt for Agent-1 (Predictor)."""
        return f"""You are an accurate and reliable AI assistant that can answer questions with the help of external documents. You should only provide the correct answer without repeating the question and instruction.

Document: {document_text}

Question: {query}

Answer:"""

    def _create_agent2_prompt(self, query, document_text, answer):
        """Create prompt for Agent-2 (Judge)."""
        return f"""You are a noisy document evaluator that can judge if the external document is noisy for the query with unrelated or misleading information. Given a retrieved Document, a Question, and an Answer generated by an LLM (LLM Answer), you should judge whether both the following two conditions are reached: (1) the Document provides specific information for answering the Question; (2) the LLM Answer directly answers the question based on the retrieved Document. Please note that external documents may contain noisy or factually incorrect information. If the information in the document does not contain the answer, you should point it out with evidence. You should answer with "Yes" or "No" with evidence of your judgment, where "No" means one of the conditions (1) and (2) are unreached and indicates it is a noisy document.

Document: {document_text}

Question: {query}

LLM Answer: {answer}

Is this document relevant and supportive for answering the question?"""

    def _create_agent3_prompt(self, query, filtered_documents):
        """
        Create prompt for Agent-3 (Final-Predictor) optimized for 300-word evaluation.
        Instructs the model to put the most important information first.
        """
        # Format documents with index
        docs_text = "\n\n".join(
            [
                f"Document {i+1}: {doc_text}"
                for i, (doc_text, _) in enumerate(filtered_documents)
            ]
        )

        return f"""You are an accurate and reliable AI assistant that can answer questions with the help of external documents. Your answer should be based ONLY on the provided documents. If the documents don't contain sufficient information to answer the question, state that clearly.

IMPORTANT FORMATTING INSTRUCTIONS:
1. PUT YOUR MOST IMPORTANT AND DIRECT ANSWERS IN THE FIRST 300 WORDS. Only the first 300 words will be evaluated.
2. Begin with a concise, direct answer to the question that captures the essential information.
3. Structure your answer with the most relevant information first, followed by supporting details.
4. Be comprehensive but prioritize clarity and relevance in the opening paragraphs.

CITATION INSTRUCTIONS:
1. For each claim or statement in your answer, add a citation in square brackets [X] immediately after the sentence.
2. DO NOT refer to documents in the text like "Document 1 states..." or "According to Document 2...".
3. Use ONLY the [X] format where X is the document number.
4. If multiple documents support a claim, cite all of them like [1,3,5].
5. Every factual claim must have a citation.

Documents:
{docs_text}

Question: {query}

Answer (with citations):"""

    def _create_agent4_prompt(self, query, answer_with_citations, filtered_documents):
        """
        Create prompt for Agent-4 (Claim Judge).
        This agent judges if the cited claims properly answer the question and identifies gaps.
        """
        # Format documents with index
        docs_text = "\n\n".join(
            [
                f"Document {i+1}: {doc_text}"
                for i, (doc_text, _) in enumerate(filtered_documents)
            ]
        )

        return f"""You are a meticulous judge evaluating if a question has been fully answered. Your task has three parts:

1. CLAIM ANALYSIS: Analyze the answer with citations and identify each claim made.
2. QUESTION COVERAGE: Determine if the original question has been completely answered.
3. GAP IDENTIFICATION: If parts of the question remain unanswered, identify these gaps precisely.

Original Question: {query}

Answer with Citations:
{answer_with_citations}

Available Documents:
{docs_text}

First, analyze each claim with its citation and evaluate if it correctly addresses part of the question.
Then, determine if any parts of the question remain unanswered.

Finally, provide your analysis in this format:
COMPLETELY_ANSWERED: Yes/No
UNANSWERED_ASPECTS: [List any aspects of the question that remain unanswered]
FOLLOW_UP_QUESTIONS: [If needed, rewrite specific questions to address the unanswered aspects]

Your analysis:"""

    def _create_follow_up_prompt(
        self, original_query, follow_up_question, filtered_documents, previous_answer
    ):
        """Create prompt for generating an answer to a follow-up question."""
        # Format documents with index
        docs_text = "\n\n".join(
            [
                f"Document {i+1}: {doc_text}"
                for i, (doc_text, _) in enumerate(filtered_documents)
            ]
        )

        return f"""You are an accurate and reliable AI assistant. Your task is to answer a follow-up question based on the provided documents.

Original Question: {original_query}
Previous Answer: {previous_answer}

Follow-up Question: {follow_up_question}

Documents:
{docs_text}

Answer the follow-up question ONLY based on the provided documents. Use the format [X] to cite which document(s) support each claim, where X is the document number. If the documents don't contain the necessary information, state that clearly.

Follow-up Answer (with citations):"""

    def _extract_claims_with_citations(self, answer_with_citations):
        """Extract individual claims and their citations from the answer."""
        # Simple regex-based extraction
        claim_pattern = r"(.*?)\s*\[(\d+(?:,\s*\d+)*)\]"
        claims = []

        for match in re.finditer(claim_pattern, answer_with_citations):
            claim_text = match.group(1).strip()
            citation_str = match.group(2)
            citations = [int(c.strip()) for c in citation_str.split(",")]

            if claim_text:
                claims.append({"text": claim_text, "citations": citations})

        return claims

    def _combine_answers(self, original_answer, follow_up_answers):
        """Combine the original answer with follow-up answers."""
        if not follow_up_answers:
            return original_answer

        combined = original_answer + "\n\nAdditional information:\n"
        combined += "\n\n".join(follow_up_answers)

        return combined

    def _remove_citations(self, answer_with_citations):
        """Remove citation brackets from the answer for final output."""
        # Remove [X] or [X,Y,Z] patterns
        return re.sub(r"\s*\[\d+(?:,\s*\d+)*\]", "", answer_with_citations)

    def clean_answer(self, answer):
        """Simple cleaning function to handle empty or problematic answers."""
        if not answer or answer.strip() == "":
            return "no answer provided"

        # Remove common problematic patterns
        patterns_to_remove = [
            r"^>.*$",  # Blog formatting
            r"^Post your responses.*$",  # Instructions
            r"^Source\(s\):.*$",  # Sources
            r"^Question:.*$",  # Follow-up questions
        ]

        for pattern in patterns_to_remove:
            answer = re.sub(pattern, "", answer, flags=re.MULTILINE)

        # Split by newlines and take all non-empty lines
        lines = [line.strip() for line in answer.split("\n") if line.strip()]
        if not lines:
            return "no answer provided"

        return "\n".join(lines)  # Return all lines to preserve structure

    def format_supporting_passages(self, filtered_docs, claims=None):
        """
        Format supporting passages in decreasing order of importance.
        If claims are provided, sort passages by how many claims they support.
        Otherwise, sort by the original document scores.
        """
        if claims:
            # Count how many claims each document supports
            doc_citation_counts = {}
            for claim in claims:
                for doc_idx in claim["citations"]:
                    if doc_idx <= len(filtered_docs):
                        doc_id = filtered_docs[doc_idx - 1][1]  # Get doc ID
                        doc_citation_counts[doc_id] = (
                            doc_citation_counts.get(doc_id, 0) + 1
                        )

            # Sort doc IDs by citation count in decreasing order
            sorted_doc_ids = sorted(
                doc_citation_counts.keys(),
                key=lambda x: doc_citation_counts[x],
                reverse=True,
            )

            # Map IDs back to (text, id) tuples
            doc_id_map = {
                doc_id: (doc_text, doc_id) for doc_text, doc_id in filtered_docs
            }
            supporting_passages = [
                doc_id_map[doc_id] for doc_id in sorted_doc_ids if doc_id in doc_id_map
            ]

            # Add any remaining documents that weren't cited
            for doc in filtered_docs:
                if doc[1] not in doc_citation_counts and doc not in supporting_passages:
                    supporting_passages.append(doc)
        else:
            # Just use the original filtered docs order
            supporting_passages = filtered_docs

        return supporting_passages

    def answer_query(self, query, choices=None):
        """Process a query using the enhanced MAIN-RAG framework with citation and judgment."""
        logger.info(f"Processing query: {query}")

        # Step 1: Retrieve documents
        logger.info("Retrieving documents...")
        retrieved_docs = self.retriever.retrieve(
            query, top_k=20
        )  # Returns [(text, id), ...]
        logger.info(f"Retrieved {len(retrieved_docs)} documents")

        # Step 2: Agent-1 generates answers for each document
        logger.info("Agent-1 generating answers for each document...")
        doc_answers = []
        for doc_text, doc_id in tqdm(retrieved_docs):
            prompt = self._create_agent1_prompt(query, doc_text)
            answer = self.agent1.generate(prompt)
            doc_answers.append((doc_text, doc_id, answer))

        # Step 3: Agent-2 evaluates and scores each document
        logger.info("Agent-2 evaluating and scoring documents...")
        scores = []
        for doc_text, doc_id, answer in tqdm(doc_answers):
            prompt = self._create_agent2_prompt(query, doc_text, answer)
            log_probs = self.agent2.get_log_probs(prompt, ["Yes", "No"])
            score = log_probs["Yes"] - log_probs["No"]  # Key scoring mechanism
            scores.append(score)

        # Step 4: Calculate adaptive judge bar (τq)
        tau_q = np.mean(scores)
        sigma = np.std(scores)
        adjusted_tau_q = tau_q - self.n * sigma  # Use the n hyperparameter here
        logger.info(
            f"Adaptive judge bar: τq={tau_q:.4f}, adjusted: {adjusted_tau_q:.4f}"
        )

        # Step 5: Filter and rank documents
        filtered_docs = []
        excluded_ids = set()

        for i, (doc_text, doc_id, _) in enumerate(doc_answers):
            if scores[i] >= adjusted_tau_q:
                filtered_docs.append((doc_text, doc_id, scores[i]))
                excluded_ids.add(doc_id)

        # Sort by score in descending order (crucial for performance)
        filtered_docs.sort(key=lambda x: x[2], reverse=True)
        filtered_docs_no_score = [
            (doc_text, doc_id) for doc_text, doc_id, _ in filtered_docs
        ]
        logger.info(f"Filtered to {len(filtered_docs)} documents")

        # Step 6: Agent-3 generates answer with citations
        logger.info("Agent-3 generating answer with citations...")
        if filtered_docs:
            prompt = self._create_agent3_prompt(query, filtered_docs_no_score)
            raw_answer = self.agent3.generate(prompt)
            answer_with_citations = self.clean_answer(raw_answer)
        else:
            # Fall back to using all documents if none pass the filter
            logger.warn("No documents passed the filter, using all documents")
            all_docs_no_score = [
                (doc_text, doc_id) for doc_text, doc_id, _ in doc_answers
            ]
            prompt = self._create_agent3_prompt(query, all_docs_no_score)
            raw_answer = self.agent3.generate(prompt)
            answer_with_citations = self.clean_answer(raw_answer)
            filtered_docs_no_score = all_docs_no_score

        # Extract claims with citations for logging
        claims = self._extract_claims_with_citations(answer_with_citations)
        logger.info(f"Extracted {len(claims)} claims with citations")

        # Log each claim with its citations
        for i, claim in enumerate(claims):
            logger.info(
                f"Claim {i+1}: {claim['text']} - cited docs: {claim['citations']}"
            )

        # Step 7: Agent-4 judges if the question is completely answered
        logger.info("Agent-4 judging answer completeness...")
        judge_prompt = self._create_agent4_prompt(
            query, answer_with_citations, filtered_docs_no_score
        )
        judge_response = self.agent4.generate(judge_prompt)
        logger.info(f"Judge response: {judge_response}")

        # Extract judgment results
        completely_answered = "COMPLETELY_ANSWERED: Yes" in judge_response
        logger.info(f"Question completely answered: {completely_answered}")

        # Process follow-up questions if needed
        follow_up_answers = []

        if not completely_answered:
            logger.info(
                "Question not completely answered. Processing follow-up questions..."
            )

            # Extract follow-up questions
            follow_up_match = re.search(
                r"FOLLOW_UP_QUESTIONS:(.*?)(?=COMPLETELY_ANSWERED:|$)",
                judge_response,
                re.DOTALL,
            )

            if follow_up_match:
                follow_up_text = follow_up_match.group(1).strip()
                follow_up_questions = [
                    q.strip() for q in follow_up_text.split("\n") if q.strip()
                ]
                logger.info(f"Extracted follow-up questions: {follow_up_questions}")

                # For each follow-up question
                for i, follow_up_q in enumerate(follow_up_questions):
                    if not follow_up_q:  # Skip empty questions
                        continue

                    logger.info(f"Processing follow-up question {i+1}: {follow_up_q}")

                    try:
                        # Retrieve new documents for follow-up question (excluding previous ids)
                        new_docs = self.retriever.retrieve(
                            follow_up_q, top_k=10, exclude_ids=excluded_ids
                        )

                        # Update excluded IDs
                        for _, doc_id in new_docs:
                            excluded_ids.add(doc_id)

                        if new_docs:
                            # Generate answer for follow-up question
                            follow_up_prompt = self._create_follow_up_prompt(
                                query, follow_up_q, new_docs, answer_with_citations
                            )
                            follow_up_answer_with_citations = self.agent3.generate(
                                follow_up_prompt
                            )

                            # Log the follow-up answer with citations
                            logger.info(
                                f"Follow-up answer (with citations): {follow_up_answer_with_citations}"
                            )

                            # Extract claims from follow-up answer
                            follow_up_claims = self._extract_claims_with_citations(
                                follow_up_answer_with_citations
                            )
                            logger.info(
                                f"Extracted {len(follow_up_claims)} claims from follow-up answer"
                            )

                            # Remove citations for the final answer
                            follow_up_answer = self._remove_citations(
                                follow_up_answer_with_citations
                            )
                            follow_up_answers.append(follow_up_answer)
                    except Exception as e:
                        logger.error(f"Error processing follow-up question: {e}")

        # Combine answers and remove citations for final answer
        answer_with_citations_for_log = answer_with_citations  # Save for logging
        final_answer = self._remove_citations(answer_with_citations)

        if follow_up_answers:
            final_answer = self._combine_answers(final_answer, follow_up_answers)

        # Make sure we never return empty answers
        if not final_answer or final_answer.strip() == "":
            final_answer = "no answer provided"

        # Format supporting passages in order of importance
        supporting_passages = self.format_supporting_passages(
            filtered_docs_no_score, claims
        )

        # Log the final answer with and without citations
        logger.info(f"Answer with citations: {answer_with_citations_for_log}")
        logger.info(f"Final answer (without citations): {final_answer}")
        logger.info(f"Supporting passages: {supporting_passages}")

        # Return the answer and debug information
        debug_info = {
            "tau_q": tau_q,
            "adjusted_tau_q": adjusted_tau_q,
            "sigma": sigma,
            "scores": scores,
            "filtered_docs": [
                (doc_text, doc_id, score) for doc_text, doc_id, score in filtered_docs
            ],
            "supporting_passages": supporting_passages,
            "raw_answer": raw_answer,
            "answer_with_citations": answer_with_citations_for_log,
            "judge_response": judge_response,
            "completely_answered": completely_answered,
            "follow_up_answers": follow_up_answers,
            "claims": claims,
            "agent3_prompt": prompt,
        }

        return final_answer, debug_info

    def extract_multiple_choice_answer(self, text):
        """Extract just the letter choice from a multiple-choice answer."""
        if not text:
            return "B"  # Default to B if empty

        text = text.strip().upper()

        # If the text starts with A, B, C, or D, take just the letter
        if text and text[0] in "ABCD":
            return text[0]

        # Otherwise search for a letter in the text
        for char in text:
            if char in "ABCD":
                return char

        # Default to B if no letter found
        return "B"
