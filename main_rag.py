import numpy as np
from tqdm import tqdm
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


class LLMAgent:
    def __init__(self, model_name="mistralai/Mistral-7B-v0.1", device="cuda"):
        """
        LLM agent for generating text and calculating probabilities.

        Args:
            model_name: Hugging Face model name
            device: Device to use (cuda or cpu)
        """
        print(f"Loading model {model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        # Optimizations for GPUs
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,  # bfloat16 for better efficiency
            device_map="auto",
        )
        self.device = device
        print("Model loaded")

    def generate(self, prompt, max_new_tokens=256):
        """Generate text using greedy decoding as used in the paper."""
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,  # Use greedy decoding as in the paper
                pad_token_id=self.tokenizer.eos_token_id,
            )
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True
        )
        return response

    def get_log_probs(self, prompt, target_tokens=["Yes", "No"]):
        """Calculate log probabilities for specific tokens."""
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.model(**inputs)

        # Get logits for the last token position
        logits = outputs.logits[0, -1, :]

        # Get token IDs for target tokens
        target_ids = []
        for token in target_tokens:
            # Handle different tokenizer behaviors
            token_ids = self.tokenizer.encode(" " + token, add_special_tokens=False)
            # Use the first token if multiple tokens
            target_ids.append(
                token_ids[0] if token_ids else self.tokenizer.unk_token_id
            )

        # Calculate log probabilities using softmax
        log_probs = torch.log_softmax(logits, dim=0)
        target_log_probs = {
            token: log_probs[tid].item()
            for token, tid in zip(target_tokens, target_ids)
        }

        return target_log_probs


class MAIN_RAG:
    def __init__(self, retriever, agent_model="mistralai/Mistral-7B-v0.1", n=0.0):
        """
        MAIN-RAG framework implementation.

        Args:
            retriever: Document retriever instance
            agent_model: Model name for the agents
            n: Hyperparameter for adaptive judge bar adjustment (default 0.0)
        """
        self.retriever = retriever
        # Initialize all agents with the same model for efficiency
        self.agent1 = LLMAgent(agent_model)  # Predictor
        self.agent2 = self.agent1  # Judge (using same instance to save memory)
        self.agent3 = self.agent1  # Final-Predictor (using same instance to save memory)
        self.n = n  # Hyperparameter for adaptive judge bar adjustment

    def _create_agent1_prompt(self, query, document):
        """Create prompt for Agent-1 (Predictor)."""
        return f"""You are an accurate and reliable AI assistant that can answer questions with the help of external documents. You should only provide the correct answer without repeating the question and instruction.

Document: {document}

Question: {query}

Answer:"""

    def _create_agent2_prompt(self, query, document, answer):
        """Create prompt for Agent-2 (Judge)."""
        return f"""You are a noisy document evaluator that can judge if the external document is noisy for the query with unrelated or misleading information. Given a retrieved Document, a Question, and an Answer generated by an LLM (LLM Answer), you should judge whether both the following two conditions are reached: (1) the Document provides specific information for answering the Question; (2) the LLM Answer directly answers the question based on the retrieved Document. Please note that external documents may contain noisy or factually incorrect information. If the information in the document does not contain the answer, you should point it out with evidence. You should answer with "Yes" or "No" with evidence of your judgment, where "No" means one of the conditions (1) and (2) are unreached and indicates it is a noisy document.

Document: {document}

Question: {query}

LLM Answer: {answer}

Is this document relevant and supportive for answering the question?"""

    def _create_agent3_prompt(self, query, filtered_documents):
        """Create prompt for Agent-3 (Final-Predictor) using the paper's format."""
        docs_text = "\n\n".join(
            [f"Document {i+1}: {doc}" for i, doc in enumerate(filtered_documents)]
        )
        return f"""You are an accurate and reliable AI assistant that can answer questions with the help of external documents. You should only provide the correct answer without repeating the question and instruction.

Documents:
{docs_text}

Question: {query}

Answer:"""

    def _create_agent3_prompt_for_multiple_choice(self, query, choices, filtered_documents):
        """Create prompt for Agent-3 (Final-Predictor) for multiple-choice questions."""
        docs_text = "\n\n".join(
            [f"Document {i+1}: {doc}" for i, doc in enumerate(filtered_documents)]
        )
        choices_text = "\n".join(
            [f"{chr(65+i)}. {choice}" for i, choice in enumerate(choices)]
        )

        return f"""You are an accurate and reliable AI assistant that can answer questions with the help of external documents. You should only provide the correct answer without repeating the question and instruction.

Documents:
{docs_text}

Question: {query}

Choices:
{choices_text}

Select the best answer choice (A, B, C, or D) based on the information in the documents.

Answer:"""

    def extract_multiple_choice_answer(self, text):
        """Extract just the letter choice from a multiple-choice answer."""
        if not text:
            return "B"  # Default to B if empty

        text = text.strip().upper()

        # If the text starts with A, B, C, or D, take just the letter
        if text and text[0] in "ABCD":
            return text[0]

        # Otherwise search for a letter in the text
        for char in text:
            if char in "ABCD":
                return char

        # Default to B if no letter found
        return "B"

    def clean_answer(self, answer):
        """Simple cleaning function to handle empty or problematic answers."""
        if not answer or answer.strip() == "":
            return "no answer provided"

        # Remove common problematic patterns
        patterns_to_remove = [
            r"^>.*$",  # Blog formatting
            r"^Post your responses.*$",  # Instructions
            r"^Source\(s\):.*$",  # Sources
            r"^Question:.*$",  # Follow-up questions
        ]

        for pattern in patterns_to_remove:
            answer = re.sub(pattern, "", answer, flags=re.MULTILINE)

        # Split by newlines and take first non-empty line
        lines = [line.strip() for line in answer.split("\n") if line.strip()]
        if not lines:
            return "no answer provided"

        # Return first substantive line
        return lines[0]

    def extract_direct_answer(self, generated_text):
        """Extract just the direct answer without formatting or source references."""
        # Remove common prefixes
        text = generated_text.strip()

        # Remove URLs and source references
        text = re.sub(r"Source:.*", "", text)
        text = re.sub(r"https?://\S+", "", text)

        # Remove document prefixes
        text = re.sub(r"Document \d+:.*?:", "", text)

        # Remove formatting instructions
        text = re.sub(r"\(Note:.*?\)", "", text)
        text = re.sub(r"##.*", "", text)

        # Split by newlines and take first substantive part
        lines = [line.strip() for line in text.split("\n") if line.strip()]
        if not lines:
            return text

        # If the first line is very short and the second has content, include both
        if len(lines) > 1 and len(lines[0]) < 15 and not lines[0].endswith("."):
            return lines[0] + " " + lines[1]

        # If there's a question mark in subsequent lines, cut there
        for i, line in enumerate(lines):
            if i > 0 and ("?" in line or line.lower().startswith("question")):
                return "\n".join(lines[:i]).strip()

        # Just return the first line in most cases
        return lines[0].strip()

    def answer_query(self, query, choices=None):
        """Process a query using the MAIN-RAG framework."""
        print(f"Processing query: {query}")

        # Step 1: Retrieve documents
        print("Retrieving documents...")
        retrieved_docs = self.retriever.retrieve(
            query, top_k=20
        )  # Always retrieve 20 docs as in the paper
        print(f"Retrieved {len(retrieved_docs)} documents")

        # Step 2: Agent-1 generates answers for each document
        print("Agent-1 generating answers for each document...")
        doc_answers = []
        for doc in tqdm(retrieved_docs):
            prompt = self._create_agent1_prompt(query, doc)
            answer = self.agent1.generate(prompt)
            doc_answers.append((doc, answer))

        # Step 3: Agent-2 evaluates and scores each document
        print("Agent-2 evaluating and scoring documents...")
        scores = []
        for doc, answer in tqdm(doc_answers):
            prompt = self._create_agent2_prompt(query, doc, answer)
            log_probs = self.agent2.get_log_probs(prompt, ["Yes", "No"])
            score = log_probs["Yes"] - log_probs["No"]  # Key scoring mechanism
            scores.append(score)

        # Step 4: Calculate adaptive judge bar (τq)
        tau_q = np.mean(scores)
        sigma = np.std(scores)
        adjusted_tau_q = tau_q - self.n * sigma  # Use the n hyperparameter here
        print(f"Adaptive judge bar: τq={tau_q:.4f}, adjusted: {adjusted_tau_q:.4f}")

        # Step 5: Filter and rank documents
        filtered_docs = []
        for i, (doc, _) in enumerate(doc_answers):
            if scores[i] >= adjusted_tau_q:
                filtered_docs.append((doc, scores[i]))

        # Sort by score in descending order (crucial for performance)
        filtered_docs.sort(key=lambda x: x[1], reverse=True)
        print(f"Filtered to {len(filtered_docs)} documents")

        # Step 6: Agent-3 generates final answer
        print("Agent-3 generating final answer...")
        if filtered_docs:
            docs_only = [doc for doc, _ in filtered_docs]

            if choices:  # Multiple choice for ARC
                prompt = self._create_agent3_prompt_for_multiple_choice(
                    query, choices, docs_only
                )
                raw_answer = self.agent3.generate(prompt)
                final_answer = self.extract_multiple_choice_answer(raw_answer)
            else:  # General prompt for all other benchmarks
                prompt = self._create_agent3_prompt(query, docs_only)
                raw_answer = self.agent3.generate(prompt)
                final_answer = self.clean_answer(raw_answer)
        else:
            # Fall back to using all documents if none pass the filter
            print("Warning: No documents passed the filter, using all documents")
            docs_only = [doc for doc, _ in doc_answers]

            if choices:
                prompt = self._create_agent3_prompt_for_multiple_choice(
                    query, choices, docs_only
                )
                raw_answer = self.agent3.generate(prompt)
                final_answer = self.extract_multiple_choice_answer(raw_answer)
            else:
                prompt = self._create_agent3_prompt(query, docs_only)
                raw_answer = self.agent3.generate(prompt)
                final_answer = self.clean_answer(raw_answer)

        # Make sure we never return empty answers
        if not final_answer or final_answer.strip() == "":
            final_answer = "no answer provided"

        # Return the answer and debug information
        debug_info = {
            "tau_q": tau_q,
            "adjusted_tau_q": adjusted_tau_q, 
            "sigma": sigma,
            "scores": scores,
            "filtered_docs": [(doc, score) for doc, score in filtered_docs],
            "raw_answer": raw_answer
        }

        return final_answer, debug_info


# Example usage with a simple retriever implementation
class SimpleRetriever:
    def __init__(self, documents=None):
        self.documents = documents or []
    
    def retrieve(self, query, top_k=10):
        """
        Simple retriever for testing. In production, replace with
        an actual retrieval system like the WikipediaRetriever from the paper.
        """
        # For testing, just return all documents (up to top_k)
        return self.documents[:top_k]

# Example usage
if __name__ == "__main__":
    # Sample documents for testing
    sample_docs = [
        "Paris is the capital city of France. The Eiffel Tower is located in Paris.",
        "The Eiffel Tower was completed in 1889 and stands 330 meters tall.",
        "France is a country in Western Europe with a population of about 67 million.",
        "The French Revolution began in 1789.",
        "The Louvre Museum is in Paris and houses the Mona Lisa painting.",
        "Berlin is the capital of Germany. It's known for the Berlin Wall.",
        "Tokyo is the capital of Japan and is the most populous metropolitan area in the world.",
        "Mount Fuji is Japan's highest mountain at 3,776 meters.",
        "The Great Wall of China is over 21,000 kilometers long.",
        "New York City has five boroughs: Manhattan, Brooklyn, Queens, The Bronx, and Staten Island."
    ]
    
    # Initialize retriever with sample documents
    retriever = SimpleRetriever(sample_docs)
    
    # Initialize MAIN-RAG
    main_rag = MAIN_RAG(retriever, agent_model="gpt2")  # Replace with your preferred model
    
    # Test query
    query = "What is the height of the Eiffel Tower?"
    answer, debug_info = main_rag.answer_query(query)
    
    print(f"\nQuery: {query}")
    print(f"Answer: {answer}")
    print(f"Adaptive Judge Bar (τq): {debug_info['tau_q']:.4f}")
    print(f"Adjusted Judge Bar: {debug_info['adjusted_tau_q']:.4f}")
    print(f"Number of filtered documents: {len(debug_info['filtered_docs'])}")
