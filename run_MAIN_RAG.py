import argparse
import json
import time
import datetime
import os
from tqdm import tqdm
import logging
import numpy as np
import random
import string
import re

# Generate a unique ID for log filename
def get_unique_log_filename():
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))
    return f"logs/main_rag_runner_{timestamp}_{random_str}.log"

# Create logs directory
os.makedirs("logs", exist_ok=True)

# Configure logging with unique filename
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler(get_unique_log_filename()), logging.StreamHandler()],
)
logger = logging.getLogger("MAIN_RAG_Runner")

# Import the retriever
from hybrid_retriever import HybridRetriever

class SimplifiedRAGENT:
    def __init__(
        self,
        retriever,
        agent_model=None,
        n=0.0,
        falcon_api_key=None,
    ):
        """
        Simplified 3-agent RAGent implementation without citation tracking.

        Args:
            retriever: Document retriever instance
            agent_model: Model name or pre-initialized agents
            n: Hyperparameter for adaptive judge bar adjustment (default 0.0)
            falcon_api_key: API key for Falcon model (if using Falcon)
        """
        self.retriever = retriever
        self.n = n  # Hyperparameter for adaptive judge bar adjustment

        # Initialize agents based on provided parameters
        if isinstance(agent_model, str):
            if "falcon" in agent_model.lower() and falcon_api_key:
                # Initialize Falcon agents
                from api_agent import FalconAgent

                self.agent1 = FalconAgent(falcon_api_key)  # Predictor
                self.agent2 = self.agent1  # Judge (reuse same instance)
                self.agent3 = self.agent1  # Final-Predictor
                logger.info(f"Using Falcon agents with API for all three agent roles")
            else:
                # Initialize local LLM agents
                from local_agent import LLMAgent

                self.agent1 = LLMAgent(agent_model)  # Predictor
                self.agent2 = self.agent1  # Judge
                self.agent3 = self.agent1  # Final-Predictor
                logger.info(f"Using local LLM agents with model {agent_model}")
        else:
            # Use pre-initialized agent
            self.agent1 = agent_model  # Predictor
            self.agent2 = self.agent1  # Judge
            self.agent3 = self.agent1  # Final-Predictor
            logger.info("Using pre-initialized agent for all three agent roles")

    def _create_agent1_prompt(self, query, document):
        """Create prompt for Agent-1 (Predictor)."""
        return f"""You are an accurate and reliable AI assistant that can answer questions with the help of external documents. You should only provide the correct answer without repeating the question and instruction.

Document: {document}

Question: {query}

Answer:"""

    def _create_agent2_prompt(self, query, document, answer):
        """Create prompt for Agent-2 (Judge)."""
        return f"""You are a noisy document evaluator that can judge if the external document is noisy for the query with unrelated or misleading information. Given a retrieved Document, a Question, and an Answer generated by an LLM (LLM Answer), you should judge whether both the following two conditions are reached: (1) the Document provides specific information for answering the Question; (2) the LLM Answer directly answers the question based on the retrieved Document. Please note that external documents may contain noisy or factually incorrect information. If the information in the document does not contain the answer, you should point it out with evidence. You should answer with "Yes" or "No" with evidence of your judgment, where "No" means one of the conditions (1) and (2) are unreached and indicates it is a noisy document.

Document: {document}

Question: {query}

LLM Answer: {answer}

Is this document relevant and supportive for answering the question?"""

    def _create_agent3_prompt(self, query, filtered_documents):
        """Create prompt for Agent-3 (Final-Predictor) using the paper's format."""
        docs_text = "\n\n".join(
            [f"Document {i+1}: {doc}" for i, doc in enumerate(filtered_documents)]
        )
        return f"""You are an accurate and reliable AI assistant that can answer questions with the help of external documents. You should only provide the correct answer without repeating the question and instruction.

Documents:
{docs_text}

Question: {query}

Answer:"""
    
    def answer_query(self, query, choices=None):
        """Process a query using a simplified 3-agent RAGent framework."""
        logger.info(f"Processing query with simplified 3-agent approach: {query}")

        # Step 1: Retrieve documents
        logger.info("Retrieving documents...")
        retrieved_docs = self.retriever.retrieve(
            query, top_k=20
        )  # Returns [(text, id), ...]
        logger.info(f"Retrieved {len(retrieved_docs)} documents")

        # Step 2: Agent-1 generates answers for each document
        logger.info("Agent-1 generating answers for each document...")
        doc_answers = []
        for doc_text, doc_id in tqdm(retrieved_docs):
            prompt = self._create_agent1_prompt(query, doc_text)
            answer = self.agent1.generate(prompt)
            doc_answers.append((doc_text, doc_id, answer))

        # Step 3: Agent-2 evaluates and scores each document
        logger.info("Agent-2 evaluating and scoring documents...")
        scores = []
        for doc_text, doc_id, answer in tqdm(doc_answers):
            prompt = self._create_agent2_prompt(query, doc_text, answer)
            log_probs = self.agent2.get_log_probs(prompt, ["Yes", "No"])
            score = log_probs["Yes"] - log_probs["No"]  # Key scoring mechanism
            scores.append(score)

        # Step 4: Calculate adaptive judge bar (τq)
        tau_q = np.mean(scores)
        sigma = np.std(scores)
        adjusted_tau_q = tau_q - self.n * sigma  # Use the n hyperparameter here
        logger.info(
            f"Adaptive judge bar: τq={tau_q:.4f}, adjusted: {adjusted_tau_q:.4f}"
        )

        # Step 5: Filter and rank documents
        filtered_docs = []
        for i, (doc_text, doc_id, _) in enumerate(doc_answers):
            if scores[i] >= adjusted_tau_q:
                filtered_docs.append((doc_text, doc_id, scores[i]))

        # Sort by score in descending order
        filtered_docs.sort(key=lambda x: x[2], reverse=True)
        filtered_docs_no_score = [
            (doc_text, doc_id) for doc_text, doc_id, _ in filtered_docs
        ]
        logger.info(f"Filtered to {len(filtered_docs)} documents")

        # Step 6: Simplified Agent-3 generates answer without citations
        logger.info("Simplified Agent-3 generating answer without citations...")
        if filtered_docs:
            prompt = self._create_agent3_prompt(query, filtered_docs_no_score)
            answer = self.agent3.generate(prompt)
        else:
            # Fall back to using top documents if none pass the filter
            logger.warn("No documents passed the filter, using top documents")
            all_docs_no_score = [
                (doc_text, doc_id) for doc_text, doc_id, _ in doc_answers[:5]  # Use top 5
            ]
            prompt = self._create_agent3_prompt(query, all_docs_no_score)
            answer = self.agent3.generate(prompt)
            filtered_docs_no_score = all_docs_no_score

        # Format supporting passages
        supporting_passages = [
            (doc_text, doc_id) for doc_text, doc_id, _ in filtered_docs
        ]

        # Log the final answer
        logger.info(f"Final answer: {answer}")

        # Return the answer and debug information
        debug_info = {
            "tau_q": tau_q,
            "adjusted_tau_q": adjusted_tau_q,
            "sigma": sigma,
            "scores": scores,
            "filtered_docs": [
                (doc_text, doc_id, score) for doc_text, doc_id, score in filtered_docs
            ],
            "supporting_passages": supporting_passages,
            "raw_answer": answer,
            "agent3_prompt": prompt,
        }

        return answer, debug_info

def load_datamorgana_questions(file_path):
    """
    Load DataMorgana questions from either JSON or JSONL format.
    """
    # Determine if file is likely JSON or JSONL based on extension
    is_jsonl = file_path.lower().endswith(".jsonl")

    try:
        questions = []

        # JSONL format: each line is a separate JSON object
        if is_jsonl:
            logger.info(f"Loading questions from JSONL file: {file_path}")
            with open(file_path, "r", encoding="utf-8") as f:
                line_num = 0
                for line in f:
                    line_num += 1
                    line = line.strip()
                    if not line:  # Skip empty lines
                        continue

                    try:
                        question = json.loads(line)

                        # Add line number as ID if not present
                        if "id" not in question:
                            question["id"] = line_num

                        questions.append(question)
                    except json.JSONDecodeError as e:
                        logger.error(f"Error parsing JSON at line {line_num}: {e}")

        # JSON format: entire file is a single JSON object or array
        else:
            logger.info(f"Loading questions from JSON file: {file_path}")
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

                # If the JSON is an array, use it directly
                if isinstance(data, list):
                    questions = data

                    # Add indices as IDs if not present
                    for i, question in enumerate(questions):
                        if "id" not in question:
                            question["id"] = i + 1
                # If the JSON is an object, look for a questions field or use as a single question
                elif isinstance(data, dict):
                    if "questions" in data:
                        questions = data["questions"]
                    elif "question" in data:
                        questions = [data]
                    else:
                        # Treat the entire object as a single question
                        questions = [data]

        logger.info(f"Loaded {len(questions)} questions")
        return questions

    except FileNotFoundError:
        logger.error(f"File not found: {file_path}")
        return []
    except Exception as e:
        logger.error(f"Unexpected error loading questions: {e}")
        return []

def format_result_to_schema(result):
    """
    Format a result dictionary to match the required schema.
    """
    # Extract the passages and document IDs
    supporting_passages = result.get("supporting_passages", [])

    # Format passages according to schema
    formatted_passages = []
    doc_passage_map = {}

    for passage_tuple in supporting_passages:
        # Each passage is a tuple of (text, doc_id)
        passage_text, doc_id = passage_tuple

        # Group passages by text to combine multiple doc_IDs
        if passage_text not in doc_passage_map:
            doc_passage_map[passage_text] = []

        if doc_id not in doc_passage_map[passage_text]:
            doc_passage_map[passage_text].append(doc_id)

    # Convert the map to the required format
    for passage_text, doc_ids in doc_passage_map.items():
        formatted_passages.append({"passage": passage_text, "doc_IDs": doc_ids})

    # Create the formatted result dictionary
    formatted_result = {
        "id": result.get("id", 0),  # Default to 0 if no ID
        "question": result.get("question", ""),
        "passages": formatted_passages,
        "final_prompt": result.get("agent3_prompt", ""),
        "answer": result.get("model_answer", ""),
    }

    return formatted_result

def write_results_to_jsonl(results, output_file):
    """Write results to JSONL file."""
    with open(output_file, "w", encoding="utf-8") as f:
        for result in results:
            formatted_result = format_result_to_schema(result)
            f.write(json.dumps(formatted_result, ensure_ascii=False) + "\n")
    logger.info(f"Results written to {output_file}")

def write_result_to_json(result, output_file):
    """Write a single result to JSON file."""
    formatted_result = format_result_to_schema(result)
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(formatted_result, f, indent=2, ensure_ascii=False)
    logger.info(f"Result written to {output_file}")

def main():
    parser = argparse.ArgumentParser(
        description="Simplified 3-Agent RAGent with Hybrid Retrieval"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="tiiuae/falcon-3-10b-instruct",
        help="Model for LLM agents",
    )
    parser.add_argument(
        "--n", type=float, default=0.5, help="Adjustment factor for adaptive judge bar"
    )
    parser.add_argument(
        "--alpha", type=float, default=0.7, help="Weight for semantic search (0-1)"
    )
    parser.add_argument(
        "--top_k", type=int, default=20, help="Number of documents to retrieve"
    )
    parser.add_argument(
        "--data_file",
        type=str,
        default="datamorgana_questions.jsonl",
        help="File containing questions (JSON or JSONL)",
    )
    parser.add_argument(
        "--single_question",
        type=str,
        default=None,
        help="Process a single question instead of the entire dataset",
    )
    parser.add_argument(
        "--output_format",
        choices=["json", "jsonl", "debug"],
        default="jsonl",
        help="Output format: 'json' for single file, 'jsonl' for line-delimited JSON, or 'debug' for detailed output",
    )
    parser.add_argument(
        "--output_dir", type=str, default="results", help="Directory to save results"
    )
    args = parser.parse_args()

    # Initialize the hybrid retriever
    logger.info(f"Initializing hybrid retriever with alpha={args.alpha}...")
    retriever = HybridRetriever(alpha=args.alpha, top_k=args.top_k)

    # Initialize SimplifiedRAGENT
    logger.info(f"Initializing simplified 3-agent RAGent with n={args.n}...")
    ragent = SimplifiedRAGENT(retriever, agent_model=args.model, n=args.n)

    # Create output directories
    os.makedirs(args.output_dir, exist_ok=True)
    os.makedirs(os.path.join(args.output_dir, "debug"), exist_ok=True)

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

    # Process a single question if specified
    if args.single_question:
        logger.info(f"\nProcessing single question: {args.single_question}")
        start_time = time.time()

        try:
            # Process the query
            answer, debug_info = ragent.answer_query(args.single_question)

            # Calculate processing time
            process_time = time.time() - start_time

            # Create result object
            result = {
                "id": "single_question",
                "question": args.single_question,
                "model_answer": answer,
                "tau_q": debug_info["tau_q"],
                "adjusted_tau_q": debug_info["adjusted_tau_q"],
                "filtered_count": len(debug_info["filtered_docs"]),
                "process_time": process_time,
                "supporting_passages": debug_info["supporting_passages"],
                "agent3_prompt": debug_info["agent3_prompt"],
            }

            logger.info(f"Answer: {answer}")
            logger.info(f"Processing time: {process_time:.2f} seconds")

            # Save result based on format
            if args.output_format == "debug":
                debug_output_file = os.path.join(
                    args.output_dir, "debug", f"simplified_single_question_debug_{timestamp}.json"
                )
                with open(debug_output_file, "w", encoding="utf-8") as f:
                    json.dump(result, f, indent=2, ensure_ascii=False)
                logger.info(f"Debug result saved to {debug_output_file}")
            else:  # json
                output_file = os.path.join(
                    args.output_dir, f"simplified_single_question_{timestamp}.json"
                )
                write_result_to_json(result, output_file)

        except Exception as e:
            logger.error(f"Error processing question: {e}", exc_info=True)

        return

    # Load questions
    questions = load_datamorgana_questions(args.data_file)
    if not questions:
        logger.error("No questions found. Exiting.")
        return

    # Process each question
    results = []

    for i, item in enumerate(questions):
        question_id = item.get("id", i + 1)
        logger.info(f"\nProcessing question {i+1}/{len(questions)}: {item['question']}")
        start_time = time.time()

        try:
            # Process the query
            answer, debug_info = ragent.answer_query(item["question"])

            # Calculate processing time
            process_time = time.time() - start_time

            # Save result
            result = {
                "id": question_id,
                "question": item["question"],
                "reference_answer": item.get("answer", ""),
                "model_answer": answer,
                "tau_q": debug_info["tau_q"],
                "adjusted_tau_q": debug_info["adjusted_tau_q"],
                "filtered_count": len(debug_info["filtered_docs"]),
                "process_time": process_time,
                "supporting_passages": debug_info["supporting_passages"],
                "agent3_prompt": debug_info["agent3_prompt"],
            }
            results.append(result)

            logger.info(f"Answer: {answer}")
            logger.info(f"Processing time: {process_time:.2f} seconds")

            # Save debug information
            debug_output_file = os.path.join(
                args.output_dir,
                "debug",
                f"simplified_question_{question_id}_debug_{timestamp}.json",
            )
            with open(debug_output_file, "w", encoding="utf-8") as f:
                json.dump(debug_info, f, indent=2, ensure_ascii=False)

        except Exception as e:
            logger.error(f"Error processing question {question_id}: {e}", exc_info=True)

    # Save all results
    if results:
        if args.output_format == "jsonl":
            output_file = os.path.join(args.output_dir, f"simplified_answers_{timestamp}.jsonl")
            write_results_to_jsonl(results, output_file)
        elif args.output_format == "json":
            # Save each result as a separate JSON file
            for result in results:
                question_id = result["id"]
                output_file = os.path.join(
                    args.output_dir, f"simplified_answer_{question_id}_{timestamp}.json"
                )
                write_result_to_json(result, output_file)
        else:  # debug
            output_file = os.path.join(
                args.output_dir, "debug", f"simplified_all_results_debug_{timestamp}.json"
            )
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            logger.info(f"Debug results saved to {output_file}")

    logger.info(f"\nProcessed {len(results)} questions.")

    # Print summary statistics
    if results:
        avg_time = sum(r["process_time"] for r in results) / len(results)
        avg_filtered = sum(r["filtered_count"] for r in results) / len(results)
        logger.info(f"Average processing time: {avg_time:.2f} seconds")
        logger.info(f"Average filtered documents: {avg_filtered:.1f}")


if __name__ == "__main__":
    main()